{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMg3EFqh6Xhw0Nwy79vM3Ls",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saivenkatreddy29/TensorFlow-Learning/blob/main/Decode_only_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NSsjeT-vW0Q5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    'I love machine learning',\n",
        "    'Transformers are Powerful',\n",
        "    'Natural Language Processing is fun',\n",
        "    'Pytorch makes Deep learning easier',\n",
        "    'Models improve with data'\n",
        "]"
      ],
      "metadata": {
        "id": "c2nGFZMMXDQ7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y2BgJu_BXoDH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(set(' '.join(sentences).split()))\n",
        "vocab_size = len(vocab)\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word,idx in word_to_idx.items()}"
      ],
      "metadata": {
        "id": "PlixjrbvXRVC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LnYOCz-XhBD",
        "outputId": "98082259-69b6-48fe-ea7f-b73e1750d2d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'are',\n",
              " 1: 'Powerful',\n",
              " 2: 'I',\n",
              " 3: 'Pytorch',\n",
              " 4: 'Models',\n",
              " 5: 'with',\n",
              " 6: 'makes',\n",
              " 7: 'Natural',\n",
              " 8: 'Language',\n",
              " 9: 'Processing',\n",
              " 10: 'easier',\n",
              " 11: 'Deep',\n",
              " 12: 'improve',\n",
              " 13: 'data',\n",
              " 14: 'Transformers',\n",
              " 15: 'is',\n",
              " 16: 'machine',\n",
              " 17: 'love',\n",
              " 18: 'learning',\n",
              " 19: 'fun'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_indices(sentence):\n",
        "  return [word_to_idx[word] for word in sentence.split()]\n",
        ""
      ],
      "metadata": {
        "id": "XW3Gah56Y8-_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = []\n",
        "output_sequence = []\n",
        "\n",
        "for sentence in sentences:\n",
        "  indices = sentence_to_indices(sentence)\n",
        "  input_sequence.append(indices[:-1])\n",
        "  output_sequence.append(indices[1:])\n",
        "\n",
        "  #padding sequences for training\n"
      ],
      "metadata": {
        "id": "aldk4QUtZP3A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwsQSJxmfOS_",
        "outputId": "500d36d2-efe8-41df-ef91-f7fe4116fcf8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 17, 16], [14, 0], [7, 8, 9, 15], [3, 6, 11, 18], [4, 12, 5]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_hU4c8kg8Yw",
        "outputId": "5369f7df-7696-4676-b599-50dec4c261c6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[17, 16, 18], [0, 1], [8, 9, 15, 19], [6, 11, 18, 10], [12, 5, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(seq) for seq in input_sequence])"
      ],
      "metadata": {
        "id": "06_NN_6Ef7M5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhjvlEJxgUQu",
        "outputId": "3f912a9b-65fc-4b21-e1f9-e11988ce55b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# max_len = max(input_sequence[0])\n",
        "padded_input = []\n",
        "padded_output = []\n",
        "for seq in input_sequence:\n",
        "    seq = seq + [0] * (max_len - len(seq))\n",
        "    padded_input.append(seq)\n",
        "for seq in output_sequence:\n",
        "    seq = seq + [0] * (max_len - len(seq))\n",
        "    padded_output.append(seq)\n"
      ],
      "metadata": {
        "id": "Bd-T8-cRfQcH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIcYhm0lfYOV",
        "outputId": "7290dfcf-ddfc-4c43-ba5c-35fcb09dc3a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 17, 16, 0], [14, 0, 0, 0], [7, 8, 9, 15], [3, 6, 11, 18], [4, 12, 5, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fFnKsXVgYC5",
        "outputId": "a30e239e-b483-463c-fdc4-49a104d837c2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[17, 16, 18, 0],\n",
              " [0, 1, 0, 0],\n",
              " [8, 9, 15, 19],\n",
              " [6, 11, 18, 10],\n",
              " [12, 5, 13, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor(padded_input)\n",
        "output = torch.tensor(padded_output)\n",
        "\n"
      ],
      "metadata": {
        "id": "-W-HUJ7Wg6dx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "5auIjn_zJPA3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Multi-Head Self-Attention Mechanism\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.nhead = nhead\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % nhead == 0, \"d_model must be divisible by nhead\"\n",
        "        self.head_dim = d_model // nhead  # Dimensionality of each attention head\n",
        "\n",
        "        # Linear layers for query, key, and value projections\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Final linear layer after attention\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "\n",
        "        # Project input to query, key, and value\n",
        "        Q = self.query_linear(x)  # Shape: (batch_size, seq_len, d_model)\n",
        "        K = self.key_linear(x)    # Shape: (batch_size, seq_len, d_model)\n",
        "        V = self.value_linear(x)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Split into multiple heads\n",
        "        Q = Q.view(batch_size, seq_len, self.nhead, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.nhead, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.nhead, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        # Concatenate all heads back\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "\n",
        "        # Final linear transformation\n",
        "        return self.out_linear(attention_output)\n"
      ],
      "metadata": {
        "id": "dVRuMq1AE_j-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Position-wise Feedforward Network\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model, hidden_dim=2048):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n"
      ],
      "metadata": {
        "id": "VxZLbEQKDI6G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding (Inject position info into embeddings)\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create a positional encoding matrix with shape (max_len, d_model)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register positional encoding as a buffer (not a parameter)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to the input embedding\n",
        "        return x + self.pe[:, :x.size(1), :]"
      ],
      "metadata": {
        "id": "w4ZNl3f9COca"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer Norm for stability\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n"
      ],
      "metadata": {
        "id": "7_NqctOEMSxK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attention = MultiHeadSelfAttention(d_model, nhead)\n",
        "        self.layer_norm1 = LayerNorm(d_model)\n",
        "        self.feed_forward = FeedForwardNetwork(d_model)\n",
        "        self.layer_norm2 = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-Attention layer with residual connection\n",
        "        attention_output = self.self_attention(x)\n",
        "        x = self.layer_norm1(x + attention_output)  # Add & norm\n",
        "\n",
        "        # Feedforward layer with residual connection\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.layer_norm2(x + ff_output)  # Add & norm\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "KCF3fMSWE8vH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder-Only Transformer\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=2, max_len=10):\n",
        "        super(DecoderOnlyTransformer, self).__init__()\n",
        "\n",
        "        self.word_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len=max_len)\n",
        "\n",
        "        # Stack multiple decoder layers\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, nhead) for _ in range(num_layers)])\n",
        "\n",
        "        # Final linear layer to predict the next word\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embed input words and add positional encoding\n",
        "        x = self.word_embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        # Pass through each decoder layer\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Output layer: Predict next word\n",
        "        return self.fc_out(x)"
      ],
      "metadata": {
        "id": "Dm9PqnTfPTHb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(vocab)  # Vocabulary size from the toy dataset\n",
        "d_model = 64  # Embedding dimension\n",
        "nhead = 4  # Number of attention heads\n",
        "num_layers = 2  # Number of decoder layers\n",
        "max_len = max_len  # Maximum sequence length (calculated before)\n",
        "learning_rate = 0.001  # Learning rate\n",
        "num_epochs = 1000  # Number of training epochs"
      ],
      "metadata": {
        "id": "d863bV93leK8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = DecoderOnlyTransformer(vocab_size=vocab_size, d_model=d_model, nhead=nhead, num_layers=num_layers, max_len=max_len)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()  # Since it's a classification task (predicting the next word)\n",
        "\n",
        "# Move model to device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "_ktLtrNmOl8s"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "AdrtlSWDc-te"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert input and output sequences to device\n",
        "input_sequences = input.to(device)\n",
        "output_sequences = output.to(device)"
      ],
      "metadata": {
        "id": "Lt0XnL0RdEbT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model in training mode\n",
        "    optimizer.zero_grad()  # Reset the gradients\n",
        "\n",
        "    # Forward pass: Input sequences are passed to the model\n",
        "    output = model(input_sequences)  # Shape: (batch_size, seq_len, vocab_size)\n",
        "\n",
        "    # Reshape the output and targets to match the loss function requirements\n",
        "    output = output.view(-1, vocab_size)  # Flatten output for CrossEntropyLoss\n",
        "    targets = output_sequences.view(-1)  # Flatten target sequences\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = loss_fn(output, targets)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}/{num_epochs}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfWZ0GI3dEXt",
        "outputId": "75e9f211-8170-454a-caa7-a846df279cd4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000, Loss: 2.967050313949585\n",
            "Epoch 100/1000, Loss: 0.016579773277044296\n",
            "Epoch 200/1000, Loss: 0.006257100962102413\n",
            "Epoch 300/1000, Loss: 0.003354964777827263\n",
            "Epoch 400/1000, Loss: 0.0021145944483578205\n",
            "Epoch 500/1000, Loss: 0.0014632362872362137\n",
            "Epoch 600/1000, Loss: 0.0010760377626866102\n",
            "Epoch 700/1000, Loss: 0.000825859431643039\n",
            "Epoch 800/1000, Loss: 0.0006542332703247666\n",
            "Epoch 900/1000, Loss: 0.0005310517735779285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, we can test the model on new input sentences.\n",
        "model.eval()  # Set the model in evaluation mode\n",
        "\n",
        "# Test the model with a new sentence\n",
        "test_sentence = \"are\"  # This is a partial input sequence\n",
        "test_input = torch.tensor([sentence_to_indices(test_sentence)], dtype=torch.long).to(device)\n"
      ],
      "metadata": {
        "id": "ItxJ13zdRPtN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the next word\n",
        "with torch.no_grad():\n",
        "    output = model(test_input)\n",
        "    predicted_index = torch.argmax(output[:, -1, :], dim=-1).item()  # Get the index of the predicted word\n",
        "    predicted_word = idx_to_word[predicted_index]  # Convert index back to word\n",
        "\n",
        "print(f'The predicted next word after \"{test_sentence}\" is \"{predicted_word}\".')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMwN_FgYSiPQ",
        "outputId": "3e460b44-b0d6-49ff-ccab-1abdaf3a59d7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted next word after \"are\" is \"Powerful\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oy_dnf-5Slh6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}